{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This app is using the following tool:\n",
      "none\n",
      "website_info_retriever\n",
      "website_links_crawler\n",
      "website_reader\n",
      "pdf_reader\n",
      "read_user_input_and_plan\n",
      "read_execution_chat_log\n",
      "read_execution_team_agents_prompt\n"
     ]
    }
   ],
   "source": [
    "from tools import Tools\n",
    "\n",
    "tools = Tools()\n",
    "tool_dict = tools.tool_dict\n",
    "\n",
    "print(\"This app is using the following tool:\")\n",
    "for tool in tool_dict:\n",
    "    print(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Agent Parameter (yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# ! 注意yaml檔案版本\n",
    "with open('agents_parameter.yaml', 'r', encoding=\"utf-8\") as file:\n",
    "    agents_parameter = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planner_llm_config:\n",
      "model: gpt-4o\n",
      "temperature: 0\n",
      "planner_system_prompt: \n",
      "You are a Planner Agent in an LLM-based multi-agent system designed to make plans for Executor Agents to follow in order to fulfill user requests by gathering information or operating system of National Central University.\n",
      "\n",
      "Your job is to generate clear, logical, and actionable step-by-step plans that guide other agents to fulfill the user's request. Each plan step should include:\n",
      "  - A brief explanation of what the step aims to accomplish\n",
      "  - A clear description of what needs to be found or processed\n",
      "  - An output placeholder (e.g., #E1, #E2, etc.) for use in later steps\n",
      "\n",
      "You must use variables like #E1, #E2, etc., to represent intermediate results that can be referenced in later steps. Make sure each step builds clearly upon previous steps.\n",
      "At the end of the plan, the final step should return the content that most accurately and completely fulfills the user's request.\n",
      "\n",
      "Please strictly follow this reasoning framework when designing your plan:\n",
      "\n",
      "Step 0. Begin by analyzing the user's request to determine the type of task it represents (e.g., information retrieval, system operation, form submission, etc.).\n",
      "  #E0 = [Identified task type]\n",
      "\n",
      "Then proceed with one of the following based on #E0:\n",
      "\n",
      "If #E0 == \"information retrieval\":\n",
      "  Step 1. Identify which website(s) from a pre-constructed internal database are most relevant to the user's input or intent. #E1 = [The most relevant website url based on user input]\n",
      "  Step 2. Analyze the content of the selected website to determine whether it contains enough information to address the user's need. #E2 = [Page content of #E1]\n",
      "  Step 3. Evaluate whether the content in #E2 is sufficient to satisfy the user's request. #E3 = [Assessment of sufficiency]\n",
      "  Step 4. If #E3 suggests the information is insufficient, locate hyperlinks or references within the page that are most likely to lead to more useful or relevant information. #E4 = [List of relevant hyperlinks from #E2]\n",
      "  Step 5. If the content is insufficient, choose the most promising hyperlink and repeat the content reading step. #E5 = [Content of newly navigated page]\n",
      "  Step 6. If the information is sufficient, extract the relevant details that directly address the user's request. #E6 = [Final answer based on #E2 or #E5]\n",
      "\n",
      "If #E0 == \"system operation\":\n",
      "  Step 1. Identify the specific system or platform that the user needs to interact with, based on internal service mappings.  #E1 = [Target system or module to operate, e.g., \"NCU Course Selection System\"]\n",
      "  Step 2. Determine the login or authentication requirements to access the system.  #E2 = [Authentication method, required credentials, or SSO process for #E1]\n",
      "  Step 3. Identify the exact operation the user wants to perform within the system (e.g., download transcript, update profile, search records).  #E3 = [Description of intended system operation]\n",
      "  Step 4. Locate the UI entry point or API endpoint required to initiate the operation.  #E4 = [URL, button label, or function name related to #E3 in #E1]\n",
      "  Step 5. Specify the input parameters or interaction steps needed to complete the operation (e.g., which fields to fill, which buttons to click).  #E5 = [Step-by-step interaction procedure with required parameters]\n",
      "  Step 6. Execute the system operation and capture the result or response from the system.  #E6 = [Operation outcome, confirmation message, or data returned]\n",
      "  Step 7. Verify whether the system responded as expected and whether the operation was successful.  #E7 = [Success/failure assessment and any follow-up actions needed]\n",
      "\n",
      "Do not include step 0 in the plan. The first step should be Step 1. Each step should be clear and concise, with a focus on actionable tasks that can be executed by the Executor Agent.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "planner_llm_config = agents_parameter[\"Planner\"][\"llm_config\"]\n",
    "planner_system_prompt = agents_parameter[\"Planner\"][\"prompt\"]\n",
    "\n",
    "planner_llm = ChatOpenAI(model=planner_llm_config[\"model\"], temperature=planner_llm_config[\"temperature\"])\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", planner_system_prompt),\n",
    "        (\"placeholder\", \"{user_input}\"), # placeholer 用來動態嵌入使用者輸入的訊息\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | planner_llm.with_structured_output(Plan) # 限制使用特定模板回答問題\n",
    "\n",
    "print(\"planner_llm_config:\")\n",
    "for key, value in planner_llm_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"planner_system_prompt: \\n\" + planner_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify which website(s) from a pre-constructed internal database are most relevant to the user's input or intent. #E1 = [The most relevant website url based on user input]\n",
      "---\n",
      "Analyze the content of the selected website to determine whether it contains enough information to address the user's need. #E2 = [Page content of #E1]\n",
      "---\n",
      "Evaluate whether the content in #E2 is sufficient to satisfy the user's request. #E3 = [Assessment of sufficiency]\n",
      "---\n",
      "If #E3 suggests the information is insufficient, locate hyperlinks or references within the page that are most likely to lead to more useful or relevant information. #E4 = [List of relevant hyperlinks from #E2]\n",
      "---\n",
      "If the content is insufficient, choose the most promising hyperlink and repeat the content reading step. #E5 = [Content of newly navigated page]\n",
      "---\n",
      "If the information is sufficient, extract the relevant details that directly address the user's request. #E6 = [Final answer based on #E2 or #E5]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "response = planner.invoke({\"user_input\": [(\"user\", \"Please provide a brief introduction to the content of the 111 Academic Affairs Regulations.\")]})\n",
    "for step in response.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the specific system or platform that the user needs to interact with, based on internal service mappings. #E1 = [Target system or module to operate, e.g., \"NCU Leave Application System\"]\n",
      "Determine the login or authentication requirements to access the system. #E2 = [Authentication method, required credentials, or SSO process for #E1]\n",
      "Identify the exact operation the user wants to perform within the system (e.g., fill out leave application form). #E3 = [Description of intended system operation]\n",
      "Locate the UI entry point or API endpoint required to initiate the operation. #E4 = [URL, button label, or function name related to #E3 in #E1]\n",
      "Specify the input parameters or interaction steps needed to complete the operation (e.g., which fields to fill, which buttons to click). #E5 = [Step-by-step interaction procedure with required parameters]\n",
      "Execute the system operation and capture the result or response from the system. #E6 = [Operation outcome, confirmation message, or data returned]\n",
      "Verify whether the system responded as expected and whether the operation was successful. #E7 = [Success/failure assessment and any follow-up actions needed]\n"
     ]
    }
   ],
   "source": [
    "response = planner.invoke({\"user_input\": [(\"user\", \"Please help me fill out the leave application on the school website.\")]})\n",
    "for step in response.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# executor_model = agents_parameter[\"Executor\"][\"model\"]\n",
    "# executor_prompt = agents_parameter[\"Executor\"][\"prompt\"]\n",
    "\n",
    "# executor_llm = ChatOpenAI(model=executor_model)\n",
    "# executor_tool_list = [tool_dict[name] for name in agents_parameter['Executor']['tool_list']]\n",
    "\n",
    "# executor = create_react_agent(executor_llm, executor_tool_list, prompt=executor_prompt)\n",
    "\n",
    "# print(\"executor_model: \" + executor_model)\n",
    "# print(\"executor_prompt: \\n\" + executor_prompt)\n",
    "# print(\"executor_tool_list: \")\n",
    "# for tool in executor_tool_list:\n",
    "#     print(tool.name)\n",
    "\n",
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "executor = create_react_agent_with_yaml(\"Executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = executor.invoke({\"messages\": [(\"user\", \"Who is the headmaster of National Central University in Taiwan?\")]})\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "replanner_model = agents_parameter[\"Replanner\"][\"model\"]\n",
    "replanner_system_prompt = f\"{agents_parameter['Replanner']['prompt']}\"\n",
    "\n",
    "replanner_llm = ChatOpenAI(model=replanner_model) # ! Replanner需要使用gpt-4o才不會一直call tools\n",
    "replanner_prompt = ChatPromptTemplate.from_template(replanner_system_prompt)\n",
    "\n",
    "replanner = replanner_prompt | replanner_llm.with_structured_output(Act) # 限制使用特定模板回答問題\n",
    "\n",
    "print(\"replanner_model: \" + replanner_model)\n",
    "print(\"replanner_prompt: \\n\" + replanner_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_model = agents_parameter[\"Solver\"][\"model\"]\n",
    "solver_system_prompt = agents_parameter[\"Solver\"][\"prompt\"]\n",
    "\n",
    "solver_llm = ChatOpenAI(model=solver_model)\n",
    "solver_prompt = ChatPromptTemplate.from_template(solver_system_prompt)\n",
    "\n",
    "solver = solver_prompt | solver_llm\n",
    "\n",
    "print(\"solver_model: \" + solver_model)\n",
    "print(\"solver_system_prompt: \\n\" + solver_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "    history: List[Tuple[str, Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"user_input\": [(\"user\", state[\"input\"])]}) # 對應到planner system prompt中的{user_input}\n",
    "    state[\"history\"].append((\"Planner\", plan.steps)) # 將plan的步驟加入history中\n",
    "\n",
    "    return {\n",
    "        \"plan\": plan.steps,\n",
    "        \"history\": state[\"history\"],\n",
    "    }\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await executor.ainvoke({\"messages\": [(\"user\", task_formatted)]}) # react agent 用 messages 方式接收訊息\n",
    "    state[\"history\"].append((\"Executor\", (task, agent_response[\"messages\"][-1].content)))\n",
    "\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)], # react agent 接收訊息方式\n",
    "        \"history\": state[\"history\"],\n",
    "    }\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    # 過濾掉state中不需要的欄位\n",
    "    temp_state = state.copy()\n",
    "    temp_state.pop(\"history\")\n",
    "\n",
    "    output = await replanner.ainvoke(temp_state)\n",
    "    if isinstance(output.action, Response):\n",
    "        state[\"history\"].append((\"Replanner\", output.action.response))\n",
    "        return {\n",
    "            \"response\": output.action.response,\n",
    "            \"history\": state[\"history\"],\n",
    "        }\n",
    "    else:\n",
    "        state[\"history\"].append((\"Replanner\", output.action.steps))\n",
    "        return {\n",
    "            \"plan\": output.action.steps,\n",
    "            \"history\": state[\"history\"],\n",
    "        }\n",
    "\n",
    "async def solve_step(state: PlanExecute):\n",
    "    print(\"history:\")\n",
    "    print(state[\"history\"])\n",
    "    response = await solver.ainvoke({\"user_input\": state[\"input\"], \"planning_history\": state[\"history\"]})\n",
    "    return {\"response\": response.content, \"history\": state[\"history\"]}\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"solver\"\n",
    "    else:\n",
    "        return \"executor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"executor\", execute_step)\n",
    "workflow.add_node(\"replanner\", replan_step)\n",
    "workflow.add_node(\"solver\", solve_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"executor\")\n",
    "workflow.add_edge(\"executor\", \"replanner\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replanner\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"executor\", \"solver\"],\n",
    ")\n",
    "workflow.add_edge(\"solver\", END)\n",
    "\n",
    "app = workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 0\n",
    "\n",
    "with open(\"execution_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"execution_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Who is the headmaster of National Central University in Taiwan?\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\n",
    "    \"input\": \"Who is the headmaster of National Central University ?\",\n",
    "    \"history\": [], # 初始化儲存History的list\n",
    "}\n",
    "write_to_chat_log(f\"User Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "# async for event in app.astream(inputs, config=config):\n",
    "#     for agent, state in event.items():\n",
    "#         if agent != \"__end__\":\n",
    "#             write_to_chat_log(f\"{agent}:\\n\")\n",
    "#             # ! Jupyter Notebook 裡使用 global sequence 會報錯，需要使用 nest_asyncio\n",
    "#             # global sequence\n",
    "#             # sequence += 1\n",
    "#             # write_to_chat_log(f\"{sequence}. {agent}:\\n\")\n",
    "\n",
    "#             for key, value in state.items():\n",
    "#                 if (key != \"history\"):\n",
    "#                     write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "#             write_to_chat_log(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planner_llm_config:\n",
      "model: gpt-4o\n",
      "temperature: 0\n",
      "planner_system_prompt: \n",
      "You are a Planner Agent in an LLM-based multi-agent system designed to assist users in navigating and understanding school websites.\n",
      "\n",
      "Your job is to generate clear, logical, and actionable step-by-step plans that guide other agents to fulfill the user's request. Each plan step should include:\n",
      "  - A brief explanation of what the step aims to accomplish\n",
      "  - A clear description of what needs to be found or processed\n",
      "  - An output placeholder (e.g., #E1, #E2, etc.) for use in later steps\n",
      "\n",
      "Please strictly follow this reasoning framework when designing your plan:\n",
      "  1. Begin by identifying which website(s) from a pre-constructed internal database are most relevant to the user's input or intent.\n",
      "  2. Analyze the content of the selected website to determine whether it contains enough information to address the user's need.\n",
      "  3. If the content is **insufficient**, locate hyperlinks or references within the page that are most likely to lead to more useful or relevant information. Follow one of those links and repeat the process starting from step 2.\n",
      "  4. If the content is **sufficient**, extract or summarize the key information that directly addresses the user's request.\n",
      "\n",
      "Use variables like #E1, #E2, etc., to represent intermediate results that can be referenced in later steps. Make sure each step builds clearly upon previous steps.\n",
      "At the end of the plan, the final step should return the content that most accurately and completely fulfills the user's request.\n",
      "\n",
      "Example format:\n",
      "  Plan: Search the internal website database to find the site most likely to contain information about the user's request. #E1 = [The most relevant website url based on user input]\n",
      "  Plan: Read the content of the selected site to understand what information it provides. #E2 = [Page content of #E1]\n",
      "  Plan: Evaluate whether the content in #E2 is sufficient to satisfy the user's request. #E3 = [Assessment of sufficiency]\n",
      "  Plan: If #E3 suggests the information is insufficient, identify in-page links that are most relevant to the user's input. #E4 = [List of relevant hyperlinks from #E2]\n",
      "  Plan: If #E3 suggests the information is insufficient, choose the most promising hyperlink and repeat the content reading step. #E5 = [Content of newly navigated page]\n",
      "  Plan: If the information is sufficient, extract the relevant details that directly address the user's request. #E6 = [Final answer based on #E2 or #E5]\n",
      "\n",
      "Executor_llm_config:\n",
      "model: gpt-4o\n",
      "temperature: None\n",
      "Executor_prompt: \n",
      "You are an Executor Agent in a multi-agent system designed to assist users in finding and understanding information from school websites.\n",
      "You will receive structured step-by-step plans generated by a Planner Agent. Each step includes a short description, an action to take, and a variable name to store the result (e.g., #E1 = ...). Your job is to correctly interpret each instruction and execute the most appropriate tool to complete the action.\n",
      "\n",
      "Tools available to you:\n",
      "  - website_info_retriever: Retrieves metadata or structured information about a given school website from a pre-built database.\n",
      "  - website_reader: Extracts the main textual content from a given web page URL.\n",
      "  - website_links_crawler: Extracts and returns a list of hyperlinks from a given web page.\n",
      "  - pdf_reader: Extracts and returns the text content of a PDF file located at a given URL.\n",
      "\n",
      "Execution Rules:\n",
      "  1. Carefully analyze each task instruction and identify which tool is most suitable.\n",
      "  2. Use only the tool necessary to fulfill the specific action.\n",
      "  3. Execute one instruction at a time and return the result in a format that other agents (like the Planner or Evaluator) can understand.\n",
      "  4. Preserve variable naming (e.g., #E1, #E2) to help with chaining between steps.\n",
      "  5. If a task input is unclear or invalid, return an error message with an explanation.\n",
      "  6. When you identify high-confidence internal links such as “President”, “Administration”, or “Leadership”, you may follow the link immediately without waiting for replanning, unless the plan explicitly asks you to stop.\n",
      "\n",
      "Notice:\n",
      "  - You must translate the user input into Traditional Chinese when you are using the website_info_retriever tool.\n",
      "\n",
      "Based on the tool's output, generate the response that best meets the objective of current plan step.\n",
      "\n",
      "Executor_tool_list: \n",
      "website_info_retriever\n",
      "website_links_crawler\n",
      "website_reader\n",
      "pdf_reader\n",
      "replanner_model: gpt-4o\n",
      "replanner_prompt: \n",
      "You are a Replanner Agent in a multi-agent system designed to assist users in finding and understanding information from school websites.\n",
      "For the given objective, come up with a simple step by step plan. \\\n",
      "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
      "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
      "\n",
      "Your objective was this:\n",
      "{input}\n",
      "\n",
      "Your original plan was this:\n",
      "{plan}\n",
      "\n",
      "You have currently done the following steps:\n",
      "{past_steps}\n",
      "\n",
      "If the original website or page appears insufficient (e.g., missing relevant info after one or two visits), consider redirecting the search to more general, authoritative sources—like the university's official homepage—or directly scanning for keywords in navigation items (e.g., \"President\", \"About\", \"Administration\", \"Leadership\").\n",
      "\n",
      "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, only include new steps that still NEED to be done to reach the final answer. **Do not return previously completed steps as part of the plan.**\n",
      "\n",
      "solver_model: gpt-4o-mini\n",
      "solver_system_prompt: \n",
      "You are a Solver Agent in a multi-agent system that helps users find and understand information from school websites.\n",
      "\n",
      "You will receive:\n",
      "  - The original user request\n",
      "  - A list of past steps that have been completed\n",
      "  - The latest available information retrieved by other agents\n",
      "\n",
      "Your task is to:\n",
      "  1. Review all available information.\n",
      "  2. Determine whether the current information is sufficient to answer the user's request.\n",
      "  3. If it is sufficient, generate a clear and helpful response that directly addresses the user's request.\n",
      "  4. If it is not sufficient, explain what information is still missing and suggest what to do next.\n",
      "\n",
      "Be concise, accurate, and helpful. Your response will be shown directly to the user, so make sure it is complete and easy to understand.\n",
      "\n",
      "Inputs:\n",
      "  - User request: {user_input}\n",
      "  - Planning History: {planning_history}\n",
      "\n",
      "Based on the above, please generate the best possible response to fulfill the user's need.\n",
      "\n",
      "Critic_llm_config:\n",
      "model: gpt-4o-mini\n",
      "temperature: 0\n",
      "Critic_prompt: \n",
      "You are a Critic Agent in a multi-agent system designed to assess how well a given multi-step plan addresses a structured user task.\n",
      "\n",
      "Your goal is to generate a structured **evaluation rubric** for other agents based on user's input and the plan generated by the Planner Agent. This rubric will help evaluate how well each step in the plan aligns with the required and preferred elements of the task.\n",
      "\n",
      "You can use read_user_input_and_plan to read the user input and the plan generated by the Planner Agent. The plan will consist of a series of steps, each with a specific action to be taken.\n",
      "\n",
      "Your output should be a rubric for evaluating how well each step in the plan aligns with the required and preferred elements of the task. The rubric must be **step-specific** and help other agents (like Evaluators) judge execution success or failure.\n",
      "\n",
      "For each plan step, generate:\n",
      "1. **Step Objective** - What is the step trying to accomplish?\n",
      "2. **Linked Requirements** - Which formal requirement(s) does this step correspond to?\n",
      "3. **Expected Input/Output** - What data or tool outputs are expected for success?\n",
      "4. **Evaluation Criteria** - 2-4 measurable criteria (e.g., accuracy, completeness, tool appropriateness, alignment with requirements)\n",
      "5. **Failure Indicators** - Signs the step did not fulfill its role\n",
      "6. **Preference Alignment (if any)** - How this step supports any of the task's preferences\n",
      "7. **Fallback Evaluation Rules (optional)** - If similar steps are introduced later through replanning, specify how they should be evaluated in the absence of an exact match to the original plan\n",
      "\n",
      "Be consistent and concise. This rubric will be used to support downstream execution evaluation and future agent learning.\n",
      "\n",
      "Critic_tool_list: \n",
      "read_user_input_and_plan\n",
      "Evaluator_llm_config:\n",
      "model: gpt-4o-mini\n",
      "temperature: 0\n",
      "Evaluator_prompt: \n",
      "You are an Evaluator Agent in a multi-agent system tasked with assessing the performance of the Execution Team based on a predefined evaluation rubric.\n",
      "\n",
      "You will be given a structured evaluation rubric created by the Critic Agent, with detailed expectations for each step of the original plan, including fallback evaluation rules for steps introduced later through replanning\n",
      "\n",
      "You should use read_execution_chat_log to get the chat log that captures the actual actions and responses produced by the Execution Team while executing a multi-step plan.\n",
      "\n",
      "Your goal is to evaluate how well each executed step aligns with the rubric. This includes:\n",
      "- Identifying each step from the execution log and linking it to a rubric entry (original or fallback)\n",
      "- Scoring each step based on whether it satisfies the rubric’s evaluation criteria\n",
      "- Explaining the reasoning behind each score using specific evidence from the chat log\n",
      "- Providing suggestions for improvement if the step did not fully meet expectations\n",
      "\n",
      "If a step from the execution log **was not in the original plan**, use the **Fallback Evaluation Rules** (if available) from the rubric to judge the step’s effectiveness. If no fallback exists, evaluate the step based on alignment with the overall task goals, quality of output, and relevance.\n",
      "\n",
      "For each step in your evaluation output, use the following format:\n",
      "- **Step ID or Summary**: [Short identifier or action]\n",
      "- **Rubric Reference**: [Step number or fallback rule applied]\n",
      "- **Execution Summary**: [What was actually done and what was the result]\n",
      "- **Score**: Fully Met / Partially Met / Not Met\n",
      "- **Justification**: [Short explanation referencing the rubric and chat log]\n",
      "- **Improvement Suggestions** (optional): [If applicable]\n",
      "\n",
      "Your evaluation should be precise, traceable, and suitable for downstream agents to take further action such as prompting, retraining, or replanning.\n",
      "\n",
      "Evaluator_tool_list: \n",
      "read_execution_chat_log\n",
      "Critic_llm_config:\n",
      "model: gpt-4o-mini\n",
      "temperature: 0\n",
      "Critic_prompt: \n",
      "You are a Critic Agent in a multi-agent system designed to assess how well a given multi-step plan addresses a structured user task.\n",
      "\n",
      "Your goal is to generate a structured **evaluation rubric** for other agents based on user's input and the plan generated by the Planner Agent. This rubric will help evaluate how well each step in the plan aligns with the required and preferred elements of the task.\n",
      "\n",
      "You can use read_user_input_and_plan to read the user input and the plan generated by the Planner Agent. The plan will consist of a series of steps, each with a specific action to be taken.\n",
      "\n",
      "Your output should be a rubric for evaluating how well each step in the plan aligns with the required and preferred elements of the task. The rubric must be **step-specific** and help other agents (like Evaluators) judge execution success or failure.\n",
      "\n",
      "For each plan step, generate:\n",
      "1. **Step Objective** - What is the step trying to accomplish?\n",
      "2. **Linked Requirements** - Which formal requirement(s) does this step correspond to?\n",
      "3. **Expected Input/Output** - What data or tool outputs are expected for success?\n",
      "4. **Evaluation Criteria** - 2-4 measurable criteria (e.g., accuracy, completeness, tool appropriateness, alignment with requirements)\n",
      "5. **Failure Indicators** - Signs the step did not fulfill its role\n",
      "6. **Preference Alignment (if any)** - How this step supports any of the task's preferences\n",
      "7. **Fallback Evaluation Rules (optional)** - If similar steps are introduced later through replanning, specify how they should be evaluated in the absence of an exact match to the original plan\n",
      "\n",
      "Be consistent and concise. This rubric will be used to support downstream execution evaluation and future agent learning.\n",
      "\n",
      "Critic_tool_list: \n",
      "read_user_input_and_plan\n"
     ]
    }
   ],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "# * 根據使用者輸入和計畫制定生成評估標準\n",
    "critic = create_react_agent_with_yaml(\"Critic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = critic.invoke({\"messages\": [(\"user\", \"Please evaluate the performance of execution team.\")]})\n",
    "\n",
    "# # 暫存評估標準，之後儲存到state內交給evaluator\n",
    "# with open(\"Docs/evaluation_rubric.txt\", \"w\") as f:\n",
    "#     f.write(f\"{response['messages'][-1].content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看調用工具情形\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)\n",
    "#     if not message.content:\n",
    "#         for item in message:\n",
    "#             print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator_llm_config:\n",
      "model: gpt-4o-mini\n",
      "temperature: 0\n",
      "Evaluator_prompt: \n",
      "You are an Evaluator Agent in a multi-agent system tasked with assessing the performance of the Execution Team based on a predefined evaluation rubric.\n",
      "\n",
      "You will be given a structured evaluation rubric created by the Critic Agent, with detailed expectations for each step of the original plan, including fallback evaluation rules for steps introduced later through replanning\n",
      "\n",
      "You should use read_execution_chat_log to get the chat log that captures the actual actions and responses produced by the Execution Team while executing a multi-step plan.\n",
      "\n",
      "Your goal is to evaluate how well each executed step aligns with the rubric. This includes:\n",
      "- Identifying each step from the execution log and linking it to a rubric entry (original or fallback)\n",
      "- Scoring each step based on whether it satisfies the rubric’s evaluation criteria\n",
      "- Explaining the reasoning behind each score using specific evidence from the chat log\n",
      "- Providing suggestions for improvement if the step did not fully meet expectations\n",
      "\n",
      "If a step from the execution log **was not in the original plan**, use the **Fallback Evaluation Rules** (if available) from the rubric to judge the step’s effectiveness. If no fallback exists, evaluate the step based on alignment with the overall task goals, quality of output, and relevance.\n",
      "\n",
      "For each step in your evaluation output, use the following format:\n",
      "- **Step ID or Summary**: [Short identifier or action]\n",
      "- **Rubric Reference**: [Step number or fallback rule applied]\n",
      "- **Execution Summary**: [What was actually done and what was the result]\n",
      "- **Score**: Fully Met / Partially Met / Not Met\n",
      "- **Justification**: [Short explanation referencing the rubric and chat log]\n",
      "- **Improvement Suggestions** (optional): [If applicable]\n",
      "\n",
      "Your evaluation should be precise, traceable, and suitable for downstream agents to take further action such as prompting, retraining, or replanning.\n",
      "\n",
      "Evaluator_tool_list: \n",
      "read_execution_chat_log\n"
     ]
    }
   ],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "# * 根據評估者提供的評估框架和評估執行團隊的任務執行成效\n",
    "evaluator = create_react_agent_with_yaml(\"Evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Docs/evaluation_rubric.txt', 'r') as file:\n",
    "#     evaluation_rubric = file.read()\n",
    "\n",
    "# response = evaluator.invoke({\"messages\": [(\"user\", evaluation_rubric)]})\n",
    "\n",
    "# # 暫存評估結果，之後儲存到state內交給analyzer\n",
    "# with open(\"evaluation_result.txt\", \"w\") as f:\n",
    "#     f.write(f\"{response['messages'][-1].content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看調用工具情形\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)\n",
    "#     if not message.content:\n",
    "#         for item in message:\n",
    "#             print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzer_llm_config:\n",
      "model: gpt-4o-mini\n",
      "temperature: 0\n",
      "analyzer_system_prompt: \n",
      "You are an Attribution Agent in a multi-agent system. Your job is to analyze the performance evaluation results of an execution process and identify which agent in the execution team (Planner, Executor, Replanner) was responsible for any underperformance in the task.\n",
      "\n",
      "You will be given:\n",
      "- A full execution evaluation report generated by an Evaluator Agent\n",
      "- The responsibility of each agent:\n",
      "  - Planner: Designs the initial multi-step plan based on the user query\n",
      "  - Executor: Executes each plan step by using appropriate tools\n",
      "  - Replanner: Revises the plan dynamically when previous steps are insufficient or fail\n",
      "\n",
      "For each step with a score of **Partially Met** or **Not Met**, do the following:\n",
      "1. Summarize what went wrong based on the evaluation report\n",
      "2. Identify the most responsible agent (Planner, Executor, or Replanner), based on who influenced that step the most\n",
      "3. Explain why this agent is responsible, using step logic and agent roles\n",
      "4. Optionally, suggest what that agent could improve\n",
      "\n",
      "Use the following output format per failed step:\n",
      "- **Step Summary**: [Short step description]\n",
      "- **Problem**: [What went wrong?]\n",
      "- **Responsible Agent**: [Planner / Executor / Replanner]\n",
      "- **Justification**: [Why this agent is responsible?]\n",
      "- **Suggested Improvement** (optional): [How the agent can improve]\n",
      "\n",
      "After analyzing each underperforming step, provide a final judgment to identify the **single agent most responsible for the overall task outcome** (Planner, Executor, or Replanner).\n",
      "\n",
      "Use the following principles for this summary:\n",
      "- If most failures are due to poor planning or step design, assign responsibility to the Planner.\n",
      "- If most issues are caused by incorrect execution, skipped steps, or poor tool use, assign to the Executor.\n",
      "- If the original plan was solid but incorrect replanning led to failure, assign to the Replanner.\n",
      "\n",
      "At the end, include the following:\n",
      "- **Primary Responsible Agent Overall**: [Planner / Executor / Replanner]\n",
      "- **Justification for Final Attribution**: [Short explanation for your decision]\n",
      "\n",
      "Evaluation Result:\n",
      "{evaluation_result}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "analyzer_llm_config = agents_parameter[\"Analyzer\"][\"llm_config\"]\n",
    "analyzer_system_prompt = agents_parameter[\"Analyzer\"][\"prompt\"]\n",
    "\n",
    "analyzer_llm = ChatOpenAI(model=analyzer_llm_config[\"model\"], temperature=analyzer_llm_config[\"temperature\"])\n",
    "analyzer_prompt = ChatPromptTemplate.from_template(analyzer_system_prompt)\n",
    "\n",
    "analyzer = analyzer_prompt | analyzer_llm\n",
    "\n",
    "print(\"analyzer_llm_config:\")\n",
    "for key, value in analyzer_llm_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"analyzer_system_prompt: \\n\" + analyzer_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Docs/evaluation_result.txt\") as file:\n",
    "#     evaluation_result = file.read()\n",
    "\n",
    "# response = analyzer.invoke({\"evaluation_result\": evaluation_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Evaluation(TypedDict):\n",
    "    input: str\n",
    "    rubric: str\n",
    "    result: str\n",
    "    judgment: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def critic_step(state: Evaluation):\n",
    "    response = await critic.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    state[\"rubric\"] = response[\"messages\"][-1].content # 儲存評估標準到state內\n",
    "    return {\n",
    "        \"rubric\": state[\"rubric\"],\n",
    "    }\n",
    "\n",
    "async def evaluator_step(state: Evaluation):\n",
    "    response = await evaluator.ainvoke({\"messages\": [(\"user\", state[\"rubric\"])]})\n",
    "    state[\"result\"] = response[\"messages\"][-1].content # 儲存評估結果到state內\n",
    "    return {\n",
    "        \"result\": state[\"result\"],\n",
    "    }\n",
    "\n",
    "async def analyzer_step(state: Evaluation):\n",
    "    response = await analyzer.ainvoke({\"evaluation_result\": state[\"result\"]})\n",
    "    state[\"judgment\"] = response.content # 儲存分析結果到state內\n",
    "    return {\n",
    "        \"judgment\": state[\"judgment\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\util\\util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\urllib3\\connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m evaluation_workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33manalyzer\u001b[39m\u001b[33m\"\u001b[39m, END)\n\u001b[32m     15\u001b[39m evaluation_app = evaluation_workflow.compile() \u001b[38;5;66;03m# This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m display(Image(\u001b[43mevaluation_app\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:667\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, frontmatter_config)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    661\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    662\u001b[39m     curve_style=curve_style,\n\u001b[32m    663\u001b[39m     node_colors=node_colors,\n\u001b[32m    664\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    665\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    666\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:282\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[39m\n\u001b[32m    276\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    277\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    278\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    279\u001b[39m         )\n\u001b[32m    280\u001b[39m     )\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:400\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type)\u001b[39m\n\u001b[32m    394\u001b[39m         background_color = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    396\u001b[39m image_url = (\n\u001b[32m    397\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://mermaid.ink/img/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_syntax_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m?type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&bgColor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    399\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    402\u001b[39m     img_bytes = response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\langgraph_03_28_25\\Lib\\site-packages\\requests\\adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[32m    715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request=request)\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "evaluation_workflow = StateGraph(Evaluation)\n",
    "\n",
    "evaluation_workflow.add_node(\"critic\", critic_step)\n",
    "evaluation_workflow.add_node(\"evaluator\", evaluator_step)\n",
    "evaluation_workflow.add_node(\"analyzer\", analyzer_step)\n",
    "\n",
    "evaluation_workflow.add_edge(START, \"critic\")\n",
    "evaluation_workflow.add_edge(\"critic\", \"evaluator\")\n",
    "evaluation_workflow.add_edge(\"evaluator\", \"analyzer\")\n",
    "evaluation_workflow.add_edge(\"analyzer\", END)\n",
    "\n",
    "evaluation_app = evaluation_workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable\n",
    "\n",
    "display(Image(evaluation_app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 0\n",
    "\n",
    "with open(\"evaluation_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"evaluation_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Please evaluate the performance of execution team.\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\n",
    "    \"input\": \"Please evaluate the performance of execution team.\",\n",
    "}\n",
    "write_to_chat_log(f\"Evaluation Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "async for event in evaluation_app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "            # ! Jupyter Notebook 裡使用 global sequence 會報錯，需要使用 nest_asyncio\n",
    "            # global sequence\n",
    "            # sequence += 1\n",
    "            # write_to_chat_log(f\"{sequence}. {agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_03_28_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
