{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools import Tools\n",
    "\n",
    "# tools = Tools()\n",
    "# tool_dict = tools.tool_dict\n",
    "\n",
    "# print(\"This app is using the following tools:\")\n",
    "# for tool in tool_dict:\n",
    "#     print(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Agent Parameter (yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "\n",
    "# # ! 注意yaml檔案版本\n",
    "# with open('agent_parameter.yaml', 'r', encoding=\"utf-8\") as file:\n",
    "#     agents_parameter = yaml.safe_load(file)\n",
    "\n",
    "# from tools import read_agents_parameter_yaml\n",
    "\n",
    "# agents_parameter = read_agents_parameter_yaml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "planner_llm_config = agents_parameter[\"Planner\"][\"llm_config\"]\n",
    "planner_system_prompt = agents_parameter[\"Planner\"][\"prompt\"]\n",
    "\n",
    "planner_llm = ChatOpenAI(model=planner_llm_config[\"model\"], temperature=planner_llm_config[\"temperature\"])\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", planner_system_prompt),\n",
    "        (\"placeholder\", \"{user_input}\"), # placeholer 用來動態嵌入使用者輸入的訊息\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | planner_llm.with_structured_output(Plan) # 限制使用特定模板回答問題\n",
    "\n",
    "print(\"planner_llm_config:\")\n",
    "for key, value in planner_llm_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"planner_system_prompt: \\n\" + planner_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = planner.invoke({\"user_input\": [(\"user\", \"Summarize the content of the 111 Academic Affairs Regulations.\")]})\n",
    "# for step in response.steps:\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = planner.invoke({\"user_input\": [(\"user\", \"Please help me fill out the leave application on the school website.\")]})\n",
    "# for step in response.steps:\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import create_react_agent_with_yaml\n",
    "\n",
    "executor = create_react_agent_with_yaml(\"Executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = executor.invoke({\"messages\": [(\"user\", \"Who is the headmaster of National Central University in Taiwan?\")]})\n",
    "for message in response[\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = executor.invoke({\"messages\": [(\"user\", \"Please help me fill out the leave application on https://cis.ncu.edu.tw/iNCU/stdAffair/leaveRequest.\")]})\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response.\"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "replanner_llm_config = agents_parameter[\"Replanner\"][\"llm_config\"]\n",
    "replanner_system_prompt = f\"{agents_parameter['Replanner']['prompt']}\"\n",
    "\n",
    "replanner_llm = ChatOpenAI(model=replanner_llm_config[\"model\"], temperature=replanner_llm_config[\"temperature\"]) # ! Replanner需要使用gpt-4o才不會一直call tools\n",
    "replanner_prompt = ChatPromptTemplate.from_template(replanner_system_prompt)\n",
    "\n",
    "replanner = replanner_prompt | replanner_llm.with_structured_output(Act) # 限制使用特定模板回答問題\n",
    "\n",
    "print(\"replanner_llm_config:\")\n",
    "for key, value in replanner_llm_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"replanner_prompt: \\n\" + replanner_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_model = agents_parameter[\"Solver\"][\"model\"]\n",
    "solver_system_prompt = agents_parameter[\"Solver\"][\"prompt\"]\n",
    "\n",
    "solver_llm = ChatOpenAI(model=solver_model)\n",
    "solver_prompt = ChatPromptTemplate.from_template(solver_system_prompt)\n",
    "\n",
    "solver = solver_prompt | solver_llm\n",
    "\n",
    "print(\"solver_model: \" + solver_model)\n",
    "print(\"solver_system_prompt: \\n\" + solver_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import ExecutionAgent\n",
    "\n",
    "execution_agents = ExecutionAgent()\n",
    "\n",
    "planner = execution_agents.planner\n",
    "executor = execution_agents.executor\n",
    "replanner = execution_agents.replanner\n",
    "solver = execution_agents.solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "    history: List[Tuple[str, Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"user_input\": [(\"user\", state[\"input\"])]}) # 對應到planner system prompt中的{user_input}\n",
    "    state[\"history\"].append((\"Planner\", plan.steps)) # 將plan的步驟加入history中\n",
    "\n",
    "    return {\n",
    "        \"plan\": plan.steps,\n",
    "        \"history\": state[\"history\"],\n",
    "    }\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await executor.ainvoke({\"messages\": [(\"user\", task_formatted)]}) # react agent 用 messages 方式接收訊息\n",
    "    state[\"history\"].append((\"Executor\", (task, agent_response[\"messages\"][-1].content)))\n",
    "\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)], # react agent 接收訊息方式\n",
    "        \"history\": state[\"history\"],\n",
    "    }\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    # 過濾掉state中不需要的欄位\n",
    "    temp_state = state.copy()\n",
    "    temp_state.pop(\"history\")\n",
    "\n",
    "    output = await replanner.ainvoke(temp_state)\n",
    "    if isinstance(output.action, execution_agents.Response):\n",
    "        state[\"history\"].append((\"Replanner\", output.action.response))\n",
    "        return {\n",
    "            \"response\": output.action.response,\n",
    "            \"history\": state[\"history\"],\n",
    "        }\n",
    "    else:\n",
    "        state[\"history\"].append((\"Replanner\", output.action.steps))\n",
    "        return {\n",
    "            \"plan\": output.action.steps,\n",
    "            \"history\": state[\"history\"],\n",
    "        }\n",
    "\n",
    "async def solve_step(state: PlanExecute):\n",
    "    print(\"history:\")\n",
    "    print(state[\"history\"])\n",
    "    response = await solver.ainvoke({\"user_input\": state[\"input\"], \"planning_history\": state[\"history\"]})\n",
    "    return {\"response\": response.content, \"history\": state[\"history\"]}\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"solver\"\n",
    "    else:\n",
    "        return \"executor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "execution_workflow = StateGraph(PlanExecute)\n",
    "\n",
    "execution_workflow.add_node(\"planner\", plan_step)\n",
    "execution_workflow.add_node(\"executor\", execute_step)\n",
    "execution_workflow.add_node(\"replanner\", replan_step)\n",
    "execution_workflow.add_node(\"solver\", solve_step)\n",
    "\n",
    "execution_workflow.add_edge(START, \"planner\")\n",
    "execution_workflow.add_edge(\"planner\", \"executor\")\n",
    "execution_workflow.add_edge(\"executor\", \"replanner\")\n",
    "execution_workflow.add_conditional_edges(\n",
    "    \"replanner\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"executor\", \"solver\"],\n",
    ")\n",
    "execution_workflow.add_edge(\"solver\", END)\n",
    "\n",
    "execution_app = execution_workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(execution_app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# start_time = time.time()\n",
    "# result = tool_dict[\"website_links_crawler\"].invoke({\"link\": \"https://pdc.adm.ncu.edu.tw/#&panel1-1\"})\n",
    "# # website_links_crawler(\"https://www.ncu.edu.tw/tw/\")\n",
    "\n",
    "\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "\n",
    "# print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import time\n",
    "\n",
    "with open(\"Outputs/execution_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"Outputs/execution_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Who is the headmaster of National Central University in Taiwan?\n",
    "# Summarize the content of the 111 Academic Affairs Regulations.\n",
    "# Please help me gather information related to scholarship applications.\n",
    "# Please help me fill out the leave application on the school website.\n",
    "config = {\"recursion_limit\": 30}\n",
    "inputs = {\n",
    "    \"input\": \"Please help me gather information related to scholarship applications.\",\n",
    "    \"history\": [], # 初始化儲存History的list\n",
    "}\n",
    "write_to_chat_log(f\"User Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "# tool_dict[\"create_browser\"].invoke(input=None)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "start_time = time.time()\n",
    "async for event in execution_app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "# del tools.selenium_controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import create_react_agent_with_yaml\n",
    "\n",
    "# * 根據使用者輸入和計畫制定生成評估標準\n",
    "critic = create_react_agent_with_yaml(\"Critic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = critic.invoke({\"messages\": [(\"user\", \"Please evaluate the performance of execution team.\")]})\n",
    "\n",
    "# # 暫存評估標準，之後儲存到state內交給evaluator\n",
    "# with open(\"Docs/evaluation_rubric.txt\", \"w\") as f:\n",
    "#     f.write(f\"{response['messages'][-1].content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看調用工具情形\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)\n",
    "#     if not message.content:\n",
    "#         for item in message:\n",
    "#             print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import create_react_agent_with_yaml\n",
    "\n",
    "# * 根據評估者提供的評估框架和評估執行團隊的任務執行成效\n",
    "evaluator = create_react_agent_with_yaml(\"Evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Docs/evaluation_rubric.txt', 'r') as file:\n",
    "#     evaluation_rubric = file.read()\n",
    "\n",
    "# response = evaluator.invoke({\"messages\": [(\"user\", evaluation_rubric)]})\n",
    "\n",
    "# # 暫存評估結果，之後儲存到state內交給analyzer\n",
    "# with open(\"evaluation_result.txt\", \"w\") as f:\n",
    "#     f.write(f\"{response['messages'][-1].content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看調用工具情形\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)\n",
    "#     if not message.content:\n",
    "#         for item in message:\n",
    "#             print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Evaluation(TypedDict):\n",
    "    input: str\n",
    "    rubric: str\n",
    "    result: str\n",
    "    judgment: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def critic_step(state: Evaluation):\n",
    "    response = await critic.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    state[\"rubric\"] = response[\"messages\"][-1].content # 儲存評估標準到state內\n",
    "    return {\n",
    "        \"rubric\": state[\"rubric\"],\n",
    "    }\n",
    "\n",
    "async def evaluator_step(state: Evaluation):\n",
    "    response = await evaluator.ainvoke({\"messages\": [(\"user\", state[\"rubric\"])]})\n",
    "    state[\"result\"] = response[\"messages\"][-1].content # 儲存評估結果到state內\n",
    "    return {\n",
    "        \"result\": state[\"result\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "evaluation_workflow = StateGraph(Evaluation)\n",
    "\n",
    "evaluation_workflow.add_node(\"critic\", critic_step)\n",
    "evaluation_workflow.add_node(\"evaluator\", evaluator_step)\n",
    "\n",
    "evaluation_workflow.add_edge(START, \"critic\")\n",
    "evaluation_workflow.add_edge(\"critic\", \"evaluator\")\n",
    "evaluation_workflow.add_edge(\"evaluator\", END)\n",
    "\n",
    "evaluation_app = evaluation_workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(evaluation_app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "with open(\"Outputs/evaluation_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"Outputs/evaluation_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Please evaluate the performance of execution team.\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\n",
    "    \"input\": \"Please evaluate the performance of execution team.\",\n",
    "}\n",
    "write_to_chat_log(f\"Evaluation Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "async for event in evaluation_app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")\n",
    "end_time = time.time()\n",
    "\n",
    "evaluation_time = end_time - start_time\n",
    "print(f\"Evaluation time: {evaluation_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# analyzer_llm_config = agents_parameter[\"Analyzer\"][\"llm_config\"]\n",
    "# analyzer_system_prompt = agents_parameter[\"Analyzer\"][\"prompt\"]\n",
    "\n",
    "# analyzer_llm = ChatOpenAI(model=analyzer_llm_config[\"model\"], temperature=analyzer_llm_config[\"temperature\"])\n",
    "# analyzer_prompt = ChatPromptTemplate.from_template(analyzer_system_prompt)\n",
    "\n",
    "# analyzer = analyzer_prompt | analyzer_llm\n",
    "\n",
    "# print(\"analyzer_llm_config:\")\n",
    "# for key, value in analyzer_llm_config.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "# print(\"analyzer_system_prompt: \\n\" + analyzer_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import create_react_agent_with_yaml\n",
    "\n",
    "analyzer = create_react_agent_with_yaml(\"Analyzer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = analyzer.invoke({\"messages\": [(\"user\", \"Please analyze the evaluation result of the execution team.\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import create_react_agent_with_yaml\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Optimization_Response(BaseModel):\n",
    "    \"\"\"Optimization response to user.\"\"\"\n",
    "    \n",
    "    updated_agent_system_prompt: str = Field(\n",
    "        description=\"The complete updated system prompt for the agent that is most responsible for the identified issue.\"\n",
    "    )\n",
    "\n",
    "prompt_optimizer = create_react_agent_with_yaml(\"Prompt Optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = \"\"\"\n",
    "analysis: All steps in the evaluation report were scored as Fully Met. There are some improvement suggestions mentioned, but none indicate clear underperformance or partial fulfillment of the task. Therefore, I will analyze the improvement suggestions to see if any step shows clear room for improvement that warrants responsibility attribution.\n",
    "\n",
    "Step 1: URL identification was appropriate; suggestion is to justify URL choice more clearly. This is a Planner-related improvement.\n",
    "\n",
    "Step 2: Content extraction was relevant; suggestion is to summarize content relevance explicitly. This relates to Executor's communication of results.\n",
    "\n",
    "Step 3: Sufficiency assessment was accurate; suggestion is to state criteria explicitly. This is a Planner responsibility to define assessment criteria.\n",
    "\n",
    "Step 4: Relevant links identified; suggestion to avoid non-functional links. This is an Executor detail in link selection.\n",
    "\n",
    "Step 5: Redirecting search was efficient; suggestion to document rationale earlier. This relates to Replanner's decision-making transparency.\n",
    "\n",
    "Step 6: Final extraction accurate; suggestion to include direct citation. This is Executor's presentation of results.\n",
    "\n",
    "Additional replanning steps: Effective replanning; suggestion to document decision-making more explicitly. This is Replanner responsibility.\n",
    "\n",
    "Summary of improvement suggestions:\n",
    "- Planner: Justify URL choice, state sufficiency criteria explicitly\n",
    "- Executor: Summarize content relevance, avoid non-functional links, include citations\n",
    "- Replanner: Document replanning decisions more explicitly\n",
    "\n",
    "None of these suggestions indicate failure or partial fulfillment, only room for clearer communication and documentation.\n",
    "\n",
    "Hence, no step shows clear underperformance. The overall task outcome was successful with all steps fully met.\n",
    "\n",
    "Final judgment:\n",
    "- No agent caused underperformance.\n",
    "- Minor improvements are distributed among Planner, Executor, and Replanner.\n",
    "- Since the plan was solid and execution was correct, and replanning was effective, the overall responsibility is balanced.\n",
    "- If forced to select the primary responsible agent for minor improvements, the Planner could be highlighted for improving clarity in plan justification and assessment criteria.\n",
    "\n",
    "---\n",
    "\n",
    "**Primary Responsible Agent**: Planner  \n",
    "**Justification for Final Attribution**: The Planner could improve by explicitly justifying URL choices and clearly stating sufficiency criteria, which would enhance clarity and reduce ambiguity in the execution process. These foundational improvements would benefit the entire workflow.  \n",
    "**Summary of Issues**: Minor suggestions for clearer documentation and communication in plan justification, content relevance assessment, and replanning rationale; no failures or partial completions.\"\"\"\n",
    "# response = prompt_optimizer.invoke({\"messages\": [(\"user\", f\"Analysis: \\n{analysis}\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Evolution(TypedDict):\n",
    "    input: str\n",
    "    analysis: str\n",
    "    result: str\n",
    "    # updated_agent_system_prompt: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_step(state: Evolution):\n",
    "    response = await analyzer.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\n",
    "        \"analysis\": response[\"messages\"][-1].content # 儲存分析結果到state內\n",
    "    }\n",
    "\n",
    "async def prompt_optimize_step(state: Evolution):\n",
    "    response = await prompt_optimizer.ainvoke({\"messages\": [(\"user\", state[\"analysis\"])]})\n",
    "    \n",
    "    return {\n",
    "        \"result\": response[\"messages\"][-1].content, # 儲存最終回覆到state內,\n",
    "        # \"updated_agent_system_prompt\": response[\"structured_response\"].updated_agent_system_prompt # 儲存更新過後的prompt到state內\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "evolution_workflow = StateGraph(Evolution)\n",
    "\n",
    "evolution_workflow.add_node(\"analyzer\", analyze_step)\n",
    "evolution_workflow.add_node(\"prompt_optimizer\", prompt_optimize_step)\n",
    "\n",
    "evolution_workflow.add_edge(START, \"analyzer\")\n",
    "evolution_workflow.add_edge(\"analyzer\", \"prompt_optimizer\")\n",
    "evolution_workflow.add_edge(\"prompt_optimizer\", END)\n",
    "\n",
    "evolution_app = evolution_workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(evolution_app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "with open(\"Outputs/evolution_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"Outputs/evolution_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Please analyze the evaluation result of the execution team.\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\n",
    "    \"input\": \"Please analyze the evaluation result of the execution team.\",\n",
    "}\n",
    "write_to_chat_log(f\"Evolution Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "async for event in evolution_app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")\n",
    "end_time = time.time()\n",
    "\n",
    "evolution_time = end_time - start_time\n",
    "print(f\"Evolution time: {evolution_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_03_28_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
