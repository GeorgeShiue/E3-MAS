{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This app is using the following tools:\n",
      "none\n",
      "website_info_retriever\n",
      "website_links_crawler\n",
      "website_reader\n",
      "pdf_reader\n",
      "create_browser\n",
      "screen_shot\n",
      "navigate\n",
      "get_html_content\n",
      "input_text_with_label\n",
      "input_text_with_name\n",
      "click_button_with_text\n",
      "click_input_with_label\n",
      "click_input_with_value\n",
      "click_input_with_id\n",
      "select_dropdown_option\n",
      "click_span_with_aria_label\n",
      "upload_file_with_id\n",
      "read_user_query_and_plan\n",
      "read_execution_chat_log\n",
      "read_execution_team_agents_prompt\n",
      "read_evaluation_result\n",
      "write_updated_agent_prompt\n"
     ]
    }
   ],
   "source": [
    "from tools import Tools\n",
    "\n",
    "tools = Tools()\n",
    "tool_dict = tools.tool_dict\n",
    "\n",
    "print(\"This app is using the following tools:\")\n",
    "for tool in tool_dict:\n",
    "    print(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Agent Parameter (yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# ! 注意yaml檔案版本\n",
    "with open('agents_parameter.yaml', 'r', encoding=\"utf-8\") as file:\n",
    "    agents_parameter = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "planner_llm_config = agents_parameter[\"Planner\"][\"llm_config\"]\n",
    "planner_system_prompt = agents_parameter[\"Planner\"][\"prompt\"]\n",
    "\n",
    "planner_llm = ChatOpenAI(model=planner_llm_config[\"model\"], temperature=planner_llm_config[\"temperature\"])\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", planner_system_prompt),\n",
    "        (\"placeholder\", \"{user_input}\"), # placeholer 用來動態嵌入使用者輸入的訊息\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | planner_llm.with_structured_output(Plan) # 限制使用特定模板回答問題\n",
    "\n",
    "print(\"planner_llm_config:\")\n",
    "for key, value in planner_llm_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"planner_system_prompt: \\n\" + planner_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = planner.invoke({\"user_input\": [(\"user\", \"Summarize the content of the 111 Academic Affairs Regulations.\")]})\n",
    "# for step in response.steps:\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = planner.invoke({\"user_input\": [(\"user\", \"Please help me fill out the leave application on the school website.\")]})\n",
    "# for step in response.steps:\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "executor = create_react_agent_with_yaml(\"Executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = executor.invoke({\"messages\": [(\"user\", \"Who is the headmaster of National Central University in Taiwan?\")]})\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = executor.invoke({\"messages\": [(\"user\", \"Please help me fill out the leave application on https://cis.ncu.edu.tw/iNCU/stdAffair/leaveRequest.\")]})\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response.\"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "replanner_model = agents_parameter[\"Replanner\"][\"model\"]\n",
    "replanner_system_prompt = f\"{agents_parameter['Replanner']['prompt']}\"\n",
    "\n",
    "replanner_llm = ChatOpenAI(model=replanner_model) # ! Replanner需要使用gpt-4o才不會一直call tools\n",
    "replanner_prompt = ChatPromptTemplate.from_template(replanner_system_prompt)\n",
    "\n",
    "replanner = replanner_prompt | replanner_llm.with_structured_output(Act) # 限制使用特定模板回答問題\n",
    "\n",
    "print(\"replanner_model: \" + replanner_model)\n",
    "print(\"replanner_prompt: \\n\" + replanner_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_model = agents_parameter[\"Solver\"][\"model\"]\n",
    "solver_system_prompt = agents_parameter[\"Solver\"][\"prompt\"]\n",
    "\n",
    "solver_llm = ChatOpenAI(model=solver_model)\n",
    "solver_prompt = ChatPromptTemplate.from_template(solver_system_prompt)\n",
    "\n",
    "solver = solver_prompt | solver_llm\n",
    "\n",
    "print(\"solver_model: \" + solver_model)\n",
    "print(\"solver_system_prompt: \\n\" + solver_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "    history: List[Tuple[str, Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"user_input\": [(\"user\", state[\"input\"])]}) # 對應到planner system prompt中的{user_input}\n",
    "    state[\"history\"].append((\"Planner\", plan.steps)) # 將plan的步驟加入history中\n",
    "\n",
    "    return {\n",
    "        \"plan\": plan.steps,\n",
    "        \"history\": state[\"history\"],\n",
    "    }\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await executor.ainvoke({\"messages\": [(\"user\", task_formatted)]}) # react agent 用 messages 方式接收訊息\n",
    "    state[\"history\"].append((\"Executor\", (task, agent_response[\"messages\"][-1].content)))\n",
    "\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)], # react agent 接收訊息方式\n",
    "        \"history\": state[\"history\"],\n",
    "    }\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    # 過濾掉state中不需要的欄位\n",
    "    temp_state = state.copy()\n",
    "    temp_state.pop(\"history\")\n",
    "\n",
    "    output = await replanner.ainvoke(temp_state)\n",
    "    if isinstance(output.action, Response):\n",
    "        state[\"history\"].append((\"Replanner\", output.action.response))\n",
    "        return {\n",
    "            \"response\": output.action.response,\n",
    "            \"history\": state[\"history\"],\n",
    "        }\n",
    "    else:\n",
    "        state[\"history\"].append((\"Replanner\", output.action.steps))\n",
    "        return {\n",
    "            \"plan\": output.action.steps,\n",
    "            \"history\": state[\"history\"],\n",
    "        }\n",
    "\n",
    "async def solve_step(state: PlanExecute):\n",
    "    print(\"history:\")\n",
    "    print(state[\"history\"])\n",
    "    response = await solver.ainvoke({\"user_input\": state[\"input\"], \"planning_history\": state[\"history\"]})\n",
    "    return {\"response\": response.content, \"history\": state[\"history\"]}\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"solver\"\n",
    "    else:\n",
    "        return \"executor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"executor\", execute_step)\n",
    "workflow.add_node(\"replanner\", replan_step)\n",
    "workflow.add_node(\"solver\", solve_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"executor\")\n",
    "workflow.add_edge(\"executor\", \"replanner\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replanner\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"executor\", \"solver\"],\n",
    ")\n",
    "workflow.add_edge(\"solver\", END)\n",
    "\n",
    "app = workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Outputs/execution_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"Outputs/execution_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Who is the headmaster of National Central University in Taiwan?\n",
    "# Summarize the content of the 111 Academic Affairs Regulations.\n",
    "# Please help me fill out the leave application on the school website.\n",
    "config = {\"recursion_limit\": 30}\n",
    "inputs = {\n",
    "    \"input\": \"Who is the headmaster of National Central University in Taiwan?\",\n",
    "    \"history\": [], # 初始化儲存History的list\n",
    "}\n",
    "write_to_chat_log(f\"User Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "# tool_dict[\"create_browser\"].invoke(input=None)\n",
    "\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")\n",
    "\n",
    "# del tools.selenium_controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor_llm_config:\n",
      "model: gpt-4.1-mini\n",
      "temperature: 0\n",
      "Executor_prompt: \n",
      "You are an Executor Agent in a multi-agent system designed to assist users in finding and understanding information from school websites.\n",
      "You will receive structured step-by-step plans generated by a Planner Agent. Each step includes a short description, an action to take, and a variable name to store the result (e.g., #E1 = ...). Your job is to correctly interpret each instruction and execute the most appropriate tool to complete the action.\n",
      "\n",
      "Tools available to you:\n",
      "  - website_info_retriever: Retrieves metadata or structured information about a given school website from a pre-built database.\n",
      "  - website_reader: Extracts the main textual content from a given web page URL.\n",
      "  - website_links_crawler: Extracts and returns a list of hyperlinks from a given web page.\n",
      "  - pdf_reader: Extracts and returns the text content of a PDF file located at a given URL.\n",
      "\n",
      "Execution Rules:\n",
      "  1. Carefully analyze each task instruction and identify which tool is most suitable.\n",
      "  2. Use only the tool necessary to fulfill the specific action.\n",
      "  3. Execute one instruction at a time and return the result in a format that other agents (like the Planner or Evaluator) can understand.\n",
      "  4. Preserve variable naming (e.g., #E1, #E2) to help with chaining between steps.\n",
      "  5. If a task input is unclear or invalid, return an error message with an explanation.\n",
      "  6. When you identify high-confidence internal links such as “President”, “Administration”, or “Leadership”, you may follow the link immediately without waiting for replanning, unless the plan explicitly asks you to stop.\n",
      "\n",
      "Notice:\n",
      "  - You must translate the user input into Traditional Chinese when you are using the website_info_retriever tool.\n",
      "\n",
      "Based on the tool's output, generate the response that best meets the objective of current plan step.\n",
      "\n",
      "Executor_tool_list: \n",
      "website_info_retriever\n",
      "website_links_crawler\n",
      "website_reader\n",
      "pdf_reader\n",
      "Critic_llm_config:\n",
      "model: gpt-4.1-mini\n",
      "temperature: 0\n",
      "Critic_prompt: \n",
      "You are a Critic Agent in a multi-agent system designed to assess how well a given multi-step plan addresses a structured user task.\n",
      "\n",
      "Your goal is to generate a structured **evaluation rubric** for other agents based on user's input and the plan generated by the Planner Agent. This rubric will help evaluate how well each step in the plan aligns with the required and preferred elements of the task.\n",
      "\n",
      "You can use read_user_query_and_plan to read the user input and the plan generated by the Planner Agent. The plan will consist of a series of steps, each with a specific action to be taken.\n",
      "\n",
      "Your output should be a rubric for evaluating how well each step in the plan aligns with the required and preferred elements of the task. The rubric must be **step-specific** and help other agents (like Evaluators) judge execution success or failure.\n",
      "\n",
      "For each plan step, generate:\n",
      "  1. **Step Objective** - What is the step trying to accomplish?\n",
      "  2. **Linked Requirements** - Which formal requirement(s) does this step correspond to?\n",
      "  3. **Expected Input/Output** - What data or tool outputs are expected for success?\n",
      "  4. **Failure Indicators** - Signs the step did not fulfill its role\n",
      "  5. **Fallback Evaluation Rules ** - If similar steps are introduced later through replanning, specify how they should be evaluated in the absence of an exact match to the original plan\n",
      "  6. **Evaluation Criteria** - several measurable criteria\n",
      "    - Information Quality: Evaluates whether the output of the step is both accurate and complete, containing all necessary information without misleading content.\n",
      "    - Alignment with Requirements: Assesses whether the execution outcome addresses the user's request and aligns with the intended goals of the task plan.\n",
      "    - Step Efficiency: Evaluates whether the step is necessary and streamlined, avoiding redundant or ineffective actions.\n",
      "    - Clarity of Expression: Checks whether the output is clearly expressed and well-structured, making it easy for downstream agents to understand and use.\n",
      "\n",
      "Be consistent and concise. This rubric will be used to support downstream execution evaluation and future agent learning.\n",
      "\n",
      "Critic_tool_list: \n",
      "read_user_query_and_plan\n"
     ]
    }
   ],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "# * 根據使用者輸入和計畫制定生成評估標準\n",
    "critic = create_react_agent_with_yaml(\"Critic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = critic.invoke({\"messages\": [(\"user\", \"Please evaluate the performance of execution team.\")]})\n",
    "\n",
    "# # 暫存評估標準，之後儲存到state內交給evaluator\n",
    "# with open(\"Docs/evaluation_rubric.txt\", \"w\") as f:\n",
    "#     f.write(f\"{response['messages'][-1].content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看調用工具情形\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)\n",
    "#     if not message.content:\n",
    "#         for item in message:\n",
    "#             print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator_llm_config:\n",
      "model: gpt-4.1-mini\n",
      "temperature: 0\n",
      "Evaluator_prompt: \n",
      "You are an Evaluator Agent in a multi-agent system tasked with assessing the performance of the Execution Team based on a predefined evaluation rubric.\n",
      "\n",
      "You will be given a structured evaluation rubric created by the Critic Agent, with detailed expectations for each step of the original plan, including fallback evaluation rules for steps introduced later through replanning.\n",
      "\n",
      "You should use read_execution_chat_log to get the chat log that captures the actual actions and responses produced by the Execution Team while executing a multi-step plan.\n",
      "\n",
      "Your goal is to evaluate how well each executed step aligns with the rubric. This includes:\n",
      "  - Identifying each step from the execution log and linking it to a rubric entry (original or fallback)\n",
      "  - For each original plan step, refer to its corresponding **Evaluation Criteria** defined in the rubric to guide your scoring and analysis\n",
      "  - Consider any **additional steps** derived from the original plan (e.g., those introduced by the Replanner) as part of the evaluation, and use fallback rules or goal alignment to assess them\n",
      "  - Scoring each step based on whether it satisfies the rubric’s evaluation criteria\n",
      "  - Explaining the reasoning behind each score using specific evidence from the chat log\n",
      "  - Providing improvement suggestions for every step, regardless of the score\n",
      "\n",
      "If a step from the execution log **was not in the original plan**, use the **Fallback Evaluation Rules** (if available) from the rubric to judge the step’s effectiveness. If no fallback exists, evaluate the step based on alignment with the overall task goals, quality of output, and relevance.\n",
      "\n",
      "For each step in your evaluation output, use the following format:\n",
      "  - Step ID or Summary: [Short identifier or action]\n",
      "  - Rubric Reference: [Step number or fallback rule applied]\n",
      "  - Execution Summary: [What was actually done and what was the result]\n",
      "  - Score: Fully Met / Partially Met / Not Met\n",
      "  - Justification: [Short explanation referencing the rubric and chat log]\n",
      "  - Improvement Suggestions: [Suggestions for how this step could be improved.]\n",
      "\n",
      "Your evaluation should be precise, traceable, and suitable for downstream agents to take further action such as prompting, retraining, or replanning.\n",
      "\n",
      "Evaluator_tool_list: \n",
      "read_execution_chat_log\n"
     ]
    }
   ],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "# * 根據評估者提供的評估框架和評估執行團隊的任務執行成效\n",
    "evaluator = create_react_agent_with_yaml(\"Evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Docs/evaluation_rubric.txt', 'r') as file:\n",
    "#     evaluation_rubric = file.read()\n",
    "\n",
    "# response = evaluator.invoke({\"messages\": [(\"user\", evaluation_rubric)]})\n",
    "\n",
    "# # 暫存評估結果，之後儲存到state內交給analyzer\n",
    "# with open(\"evaluation_result.txt\", \"w\") as f:\n",
    "#     f.write(f\"{response['messages'][-1].content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看調用工具情形\n",
    "# for message in response[\"messages\"]:\n",
    "#     print(message)\n",
    "#     if not message.content:\n",
    "#         for item in message:\n",
    "#             print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Evaluation(TypedDict):\n",
    "    input: str\n",
    "    rubric: str\n",
    "    result: str\n",
    "    judgment: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def critic_step(state: Evaluation):\n",
    "    response = await critic.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    state[\"rubric\"] = response[\"messages\"][-1].content # 儲存評估標準到state內\n",
    "    return {\n",
    "        \"rubric\": state[\"rubric\"],\n",
    "    }\n",
    "\n",
    "async def evaluator_step(state: Evaluation):\n",
    "    response = await evaluator.ainvoke({\"messages\": [(\"user\", state[\"rubric\"])]})\n",
    "    state[\"result\"] = response[\"messages\"][-1].content # 儲存評估結果到state內\n",
    "    return {\n",
    "        \"result\": state[\"result\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "evaluation_workflow = StateGraph(Evaluation)\n",
    "\n",
    "evaluation_workflow.add_node(\"critic\", critic_step)\n",
    "evaluation_workflow.add_node(\"evaluator\", evaluator_step)\n",
    "\n",
    "evaluation_workflow.add_edge(START, \"critic\")\n",
    "evaluation_workflow.add_edge(\"critic\", \"evaluator\")\n",
    "evaluation_workflow.add_edge(\"evaluator\", END)\n",
    "\n",
    "evaluation_app = evaluation_workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(evaluation_app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 23.16 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with open(\"Outputs/evaluation_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"Outputs/evaluation_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Please evaluate the performance of execution team.\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\n",
    "    \"input\": \"Please evaluate the performance of execution team.\",\n",
    "}\n",
    "write_to_chat_log(f\"Evaluation Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "async for event in evaluation_app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")\n",
    "end_time = time.time()\n",
    "\n",
    "evaluation_time = end_time - start_time\n",
    "print(f\"Evaluation time: {evaluation_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# analyzer_llm_config = agents_parameter[\"Analyzer\"][\"llm_config\"]\n",
    "# analyzer_system_prompt = agents_parameter[\"Analyzer\"][\"prompt\"]\n",
    "\n",
    "# analyzer_llm = ChatOpenAI(model=analyzer_llm_config[\"model\"], temperature=analyzer_llm_config[\"temperature\"])\n",
    "# analyzer_prompt = ChatPromptTemplate.from_template(analyzer_system_prompt)\n",
    "\n",
    "# analyzer = analyzer_prompt | analyzer_llm\n",
    "\n",
    "# print(\"analyzer_llm_config:\")\n",
    "# for key, value in analyzer_llm_config.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "# print(\"analyzer_system_prompt: \\n\" + analyzer_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "\n",
    "analyzer = create_react_agent_with_yaml(\"Analyzer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = analyzer.invoke({\"messages\": [(\"user\", \"Please analyze the evaluation result of the execution team.\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import create_react_agent_with_yaml\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Optimization_Response(BaseModel):\n",
    "    \"\"\"Optimization response to user.\"\"\"\n",
    "    \n",
    "    updated_agent_system_prompt: str = Field(\n",
    "        description=\"The complete updated system prompt for the agent that is most responsible for the identified issue.\"\n",
    "    )\n",
    "\n",
    "prompt_optimizer = create_react_agent_with_yaml(\"Prompt Optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = \"\"\"\n",
    "### Step Analysis\n",
    "\n",
    "- **Step Summary**: Evaluate content sufficiency\n",
    "  - **Problem**: The team attempted to assess the sufficiency of the information but did not find a direct link to the headmaster's information, indicating a lack of depth in their evaluation.\n",
    "  - **Responsible Agent**: Replanner\n",
    "  - **Justification**: The Replanner is responsible because they should have guided the team to explore all identified links and sections more thoroughly before concluding the sufficiency of the information. The evaluation step required a more comprehensive approach to ensure all potential sources were considered.\n",
    "  - **Suggested Improvement**: The Replanner could improve by ensuring that the team explores all identified links and sections comprehensively before concluding the sufficiency of the information.\n",
    "\n",
    "- **Step Summary**: Repeat content reading for new hyperlink\n",
    "  - **Problem**: The team faced challenges in finding a search feature on the homepage, which hindered their ability to gather comprehensive information.\n",
    "  - **Responsible Agent**: Executor\n",
    "  - **Justification**: The Executor is responsible here as they were tasked with executing the plan and adapting to the situation. The inability to find a search feature indicates a failure to effectively navigate the site and utilize available resources.\n",
    "  - **Suggested Improvement**: The Executor should develop a backup plan for situations where direct navigation or search features are unavailable, such as manually browsing through sections.\n",
    "\n",
    "### Final Judgment\n",
    "\n",
    "- **Primary Responsible Agent Overall**: Replanner\n",
    "- **Justification for Final Attribution**: While both the Replanner and Executor had issues in their respective steps, the Replanner's failure to ensure a thorough evaluation of content sufficiency and to guide the team effectively contributed significantly to the overall underperformance. The Executor's challenges were more situational, while the Replanner's oversight in planning and guidance had a broader impact on the execution process.\n",
    "\"\"\"\n",
    "# response = prompt_optimizer.invoke({\"messages\": [(\"user\", f\"Analysis: \\n{analysis}\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"structured_response\"].updated_agent_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Evolution(TypedDict):\n",
    "    input: str\n",
    "    analysis: str\n",
    "    updated_agent_system_prompt: str\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_step(state: Evolution):\n",
    "    response = await analyzer.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\n",
    "        \"analysis\": response[\"messages\"][-1].content # 儲存分析結果到state內\n",
    "    }\n",
    "\n",
    "async def prompt_optimize_step(state: Evolution):\n",
    "    response = await prompt_optimizer.ainvoke({\"messages\": [(\"user\", state[\"analysis\"])]})\n",
    "    \n",
    "    return {\n",
    "        \"result\": response[\"messages\"][-1].content, # 儲存最終回覆到state內,\n",
    "        # \"updated_agent_system_prompt\": response[\"structured_response\"].updated_agent_system_prompt # 儲存更新過後的prompt到state內\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "evolution_workflow = StateGraph(Evolution)\n",
    "\n",
    "evolution_workflow.add_node(\"analyzer\", analyze_step)\n",
    "evolution_workflow.add_node(\"prompt_optimizer\", prompt_optimize_step)\n",
    "\n",
    "evolution_workflow.add_edge(START, \"analyzer\")\n",
    "evolution_workflow.add_edge(\"analyzer\", \"prompt_optimizer\")\n",
    "evolution_workflow.add_edge(\"prompt_optimizer\", END)\n",
    "\n",
    "evolution_app = evolution_workflow.compile() # This compiles it into a LangChain Runnable, meaning you can use it as you would any other runnable\n",
    "\n",
    "# display(Image(evolution_app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Outputs/evolution_chat_log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "def write_to_chat_log(content):\n",
    "    with open(\"Outputs/evolution_chat_log.txt\", \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Please analyze the evaluation result of the execution team.\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\n",
    "    \"input\": \"Please analyze the evaluation result of the execution team.\",\n",
    "}\n",
    "write_to_chat_log(f\"Evolution Query:\\n{inputs['input']}\\n\\n\")\n",
    "\n",
    "async for event in evolution_app.astream(inputs, config=config):\n",
    "    for agent, state in event.items():\n",
    "        if agent != \"__end__\":\n",
    "            write_to_chat_log(f\"{agent}:\\n\")\n",
    "\n",
    "            for key, value in state.items():\n",
    "                if (key != \"history\"):\n",
    "                    write_to_chat_log(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            write_to_chat_log(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_03_28_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
